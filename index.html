<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Mengdi Jia</title>
    <meta name="author" content="Mengdi Jia">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0;border-spacing:0;margin:auto;">
      <tbody>
        <tr><td>

          <!-- Header -->
          <table style="width:100%;border:0;border-spacing:0;margin:auto;">
            <tr>
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-size:32px;font-weight:bold;">Mengdi Jia</p>
                <p>
                  I am currently a mechatronics engineer based in Beijing. Previously, I received my Master's degree from 
                  <a href="https://www.ahau.edu.cn/">Anhui Agricultural University</a>, advised by Prof. <a href="https://jsxx.ahau.edu.cn/ch/jsxx_show.html?zgh=1994047">Chengmao Cao</a>,
                  and my Bachelor's degree from <a href="https://www.hebau.edu.cn/">Hebei Agricultural University</a>.
                  During my research internship at <a href="https://iiis.tsinghua.edu.cn/en/">Tsinghua University</a>, I contributed to projects including
                  <a href="https://qizekun.github.io/omnispatial/">OmniSpatial</a> and CTRL, working closely with
                  <a href="https://qizekun.github.io/">Zekun Qi</a> and <a href="https://ericyi.github.io/">Li Yi</a>.
                </p>
                <p>As an independent researcher, my primary focus is on embodied intelligence and spatial reasoning.</p>
                <p style="text-align:center">
                  <a href="mailto:j.m.d.chn@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/MengdiJia-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://au.cnki.net/author/personalInfo/000060962837?platform=kns-author">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MengdiJia">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/0.jpg">
                  <img style="width:100%;object-fit:cover;border-radius:8px;" alt="profile photo" src="images/0.jpg">
                </a>
              </td>
            </tr>
          </table>

          <!-- News -->
          <table style="width:100%;border:0;border-spacing:0;margin:auto;">
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li><b>2025-07</b>: One paper <i>submitted to</i> NeurIPS 2025.</li>
                  <li><b>2025-09</b>: Another paper <i>submitted</i>.</li>
                </ul>
              </td>
            </tr>
          </table>

          <!-- Publications -->
          <table style="width:100%;border:0;border-spacing:0;margin:auto;">
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p><i>* indicates equal contribution</i></p>
              </td>
            </tr>
          </table>

          <table style="width:100%;border:0;border-spacing:0 10px;margin:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <img src="images/radar.png" width="160">
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://qizekun.github.io/omnispatial/">
                    <span class="papertitle">OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models</span>
                  </a>
                  <br>
                  <strong>Mengdi Jia</strong>, Zekun Qi*, Shaochen Zhang, Wenyao Zhang, XinQiang Yu, Jiawei He, He Wang, Li Yi
                  <br><i>Under review at NeurIPS 2025</i>
                  <br><a href="https://qizekun.github.io/omnispatial/">Project Page</a> / <a href="#">arXiv</a>
                  <p>
                    We present OmniSpatial, a benchmark for evaluating spatial reasoning abilities in vision-language models.
                    It covers four major categories and 50+ subtypes, totaling over 1.5K QA pairs. Experiments reveal significant
                    limitations in existing models and suggest directions for improvement.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Footer -->
          <table style="width:100%;margin:auto;border:0;">
            <tr>
              <td style="padding:0;">
                <br>
                <p style="text-align:right;font-size:small;">
<!--                   Website template adapted from <a href="https://jonbarron.info">Jon Barron's academic page</a>.
                  Source code: <a href="https://github.com/jonbarron/jonbarron.github.io">GitHub</a> -->
                </p>
              </td>
            </tr>
          </table>

        </td></tr>
      </tbody>
    </table>
  </body>
</html>
