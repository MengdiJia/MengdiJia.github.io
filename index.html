<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Mengdi Jia</title>
    <meta name="author" content="Mengdi Jia">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0;border-spacing:0;margin:auto;">
      <tbody>
        <tr><td>

          <!-- Header -->
          <table style="width:100%;border:0;border-spacing:0;margin:auto;">
            <tr>
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align:center;font-size:32px;font-weight:bold;">Mengdi Jia</p>
                <p>
                  I am currently a mechatronics engineer based in Beijing. Previously, I received my Master's degree from 
                  <a href="https://www.ahau.edu.cn/">Anhui Agricultural University</a>, advised by Prof. <a href="https://jsxx.ahau.edu.cn/ch/jsxx_show.html?zgh=1994047">Chengmao Cao</a>,
                  and my Bachelor's degree from <a href="https://www.hebau.edu.cn/">Hebei Agricultural University</a>.
                  During my research internship at <a href="https://iiis.tsinghua.edu.cn/en/">Tsinghua University</a>, I contributed to projects including
                  <a href="https://qizekun.github.io/omnispatial/">OmniSpatial</a> and CTRL, working closely with
                  <a href="https://qizekun.github.io/">Zekun Qi</a>.
                </p>
                <p>My primary focus is on Embodied Intelligence, and Multimodal Large Language Models.</p>
                <p style="text-align:center">
                  <a href="mailto:mengdi__jia@163.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://au.cnki.net/author/personalInfo/000060962837?platform=kns-author">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MengdiJia">GitHub</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a>
                </p>
              </td>

              <!-- 方形头像 -->
<!--               <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/1.jpg">
                  <img style="width:100%;object-fit:cover;border-radius:8px;" alt="profile photo" src="images/1.jpg"> -->

              <!-- 圆形头像 -->
              <td style="padding:2.5%;width:37%;max-width:37%;text-align:center;">
                <img style="width:100%;max-width:200px;object-fit:cover;border-radius:50%;" 
                     src="images/1.jpg" alt="profile photo">
              </td>

            </tr>
          </table>

          <!-- Publications -->
                    <table style="width:100%;border:0;border-spacing:0;margin:auto;">
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p><i>* indicates equal contribution</i></p>
              </td>
            </tr>
          </table>


          
<table style="width:100%;border:0;border-spacing:0 10px;margin:auto;">
  <tbody>
    <tr>
      <td style="padding:16px;width:30%;vertical-align:middle"> <!-- 视频尺寸增大至30% -->
        <video width="100%" autoplay muted loop playsinline style="pointer-events:none; border-radius:8px;">
          <source src="data/OmniSpatial.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>

      <td style="padding:8px;width:70%;vertical-align:middle">
        <span class="papertitle">OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models</span>
        <br>
        <strong>Mengdi Jia*</strong>, 
        <a href="https://qizekun.github.io/">Zekun Qi*</a>, 
        <a href="">Shaochen Zhang</a>, 
        <a href="https://scholar.google.com/citations?user=w_Szx5MAAAAJ">Wenyao Zhang*</a>, 
        <a href="">XinQiang Yu</a>, 
        <a href="https://jiaweihe.com/">Jiawei He</a>, 
        <a href="https://hughw19.github.io/">He Wang</a>, 
        <a href="https://ericyi.github.io/">Li Yi</a>

        <br><i>arXiv preprint, 2025</i>
        <p>
          We present OmniSpatial, a benchmark for evaluating spatial reasoning abilities in vision-language models.
          It covers four major categories and 50+ subtypes, totaling over 1.5K QA pairs. Experiments reveal significant
          limitations in existing models and suggest directions for improvement.
        </p>
        <p>
          <a href="https://arxiv.org/abs/2506.03135">[arXiv]</a>
          <a href="https://qizekun.github.io/omnispatial/">[Project Page]</a>
          <a href="https://github.com/qizekun/OmniSpatial">[Code]</a>
          <a href="https://huggingface.co/datasets/qizekun/OmniSpatial">[Huggingface]</a>
        </p>
      </td>
    </tr>
  </tbody>
</table>



          <!-- OmniSpatial Video -->
          <table style="width:100%;margin:auto;border:0;">
            <tr>
              <td style="padding:0;">
                <video width="100%" controls>
                  <source src="data/OmniSpatial.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </td>
            </tr>
          </table>

          <!-- Footer -->
          <table style="width:100%;margin:auto;border:0;">
            <tr>
              <td style="padding:0;">
                <br>
              </td>
            </tr>
          </table>

        </td></tr>
      </tbody>
    </table>
  </body>
</html>
